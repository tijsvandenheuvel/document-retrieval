{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamaindex create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tijsvandenheuvel/.virtualenvs/doc_retrieval_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from db_opensearch import opensearch_client\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pympler import asizeof\n",
    "from llama_index.core.node_parser import SimpleNodeParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents_from_opensearch(client, index_name, num_of_docs):\n",
    "    search_body = {\n",
    "        \"query\": {\"match_all\": {}},  # Fetch all documents\n",
    "        \"_source\": [\"content\", \"content_vector\", \"file_path\"]\n",
    "    }\n",
    "    response = client.search(index=index_name, body=search_body, size=num_of_docs)\n",
    "\n",
    "    documents = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        source = hit['_source']\n",
    "        documents.append({\n",
    "            \"content\": source[\"content\"],\n",
    "            \"vector\": source[\"content_vector\"],  # Vector embeddings\n",
    "            \"metadata\": {\"file_path\": source.get(\"file_path\", \"\")}  # Add any metadata needed\n",
    "        })\n",
    "        \n",
    "    return documents\n",
    "\n",
    "def format_bytes(size):\n",
    "    for unit in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size < 1024.0:\n",
    "            return f\"{size:.2f} {unit}\"\n",
    "        size /= 1024.0\n",
    "        \n",
    "def format_seconds(seconds):\n",
    "    days = int(seconds // 86400)\n",
    "    seconds %= 86400\n",
    "    hours = int(seconds // 3600)\n",
    "    seconds %= 3600\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds %= 60\n",
    "\n",
    "    result = []\n",
    "    if days > 0:\n",
    "        result.append(f\"{days} days\")\n",
    "    if hours > 0:\n",
    "        result.append(f\"{hours} hours\")\n",
    "    if minutes > 0:\n",
    "        result.append(f\"{minutes} minutes\")\n",
    "    result.append(f\"{seconds:.2f} seconds\")\n",
    "\n",
    "    return \", \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config llamindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n",
      "Embedding model: dunzhang/stella_en_1.5B_v5\n",
      "Chunk size: 4096\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 4096\n",
    "Settings.chunk_size = chunk_size\n",
    "\n",
    "Settings.llm = None\n",
    "\n",
    "model_name=\"dunzhang/stella_en_1.5B_v5\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(f\"Embedding model: {model_name}\")\n",
    "print(f\"Chunk size: {chunk_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 4695\n"
     ]
    }
   ],
   "source": [
    "documents = fetch_documents_from_opensearch(opensearch_client, \"documents\", 10000)\n",
    "    \n",
    "print(f\"Documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4695\n",
      "46814\n"
     ]
    }
   ],
   "source": [
    "llama_documents = [\n",
    "        Document(\n",
    "            text=doc[\"content\"],\n",
    "            metadata=doc[\"metadata\"]\n",
    "        )\n",
    "        for doc in documents\n",
    "    ]\n",
    "print(len(llama_documents))\n",
    "\n",
    "node_parser = SimpleNodeParser()\n",
    "nodes = node_parser.get_nodes_from_documents(llama_documents)\n",
    "\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "embedding_model = HuggingFaceEmbedding(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - Indexing time: 13.37 seconds\n",
      "Batch 2 - Indexing time: 10.45 seconds\n",
      "Batch 3 - Indexing time: 9.12 seconds\n",
      "Batch 4 - Indexing time: 9.18 seconds\n",
      "Batch 5 - Indexing time: 9.55 seconds\n",
      "Batch 6 - Indexing time: 9.72 seconds\n",
      "Batch 7 - Indexing time: 8.54 seconds\n",
      "Batch 8 - Indexing time: 9.24 seconds\n",
      "Batch 9 - Indexing time: 9.18 seconds\n",
      "Batch 10 - Indexing time: 9.20 seconds\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all nodes in batches\n",
    "\n",
    "# node_texts = [node.text for node in nodes]\n",
    "# batch_size = 100 \n",
    "\n",
    "node_texts = [node.text for node in nodes][:100]\n",
    "batch_size = 10 \n",
    "embeddings = []\n",
    "\n",
    "for i in range(0, len(node_texts), batch_size):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    batch = node_texts[i:i + batch_size]\n",
    "    batch_embeddings = embedding_model.get_text_embedding_batch(batch)  # Use batch embedding\n",
    "    embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Batch {i//batch_size + 1} - Indexing time: {format_seconds(elapsed_time)}\")\n",
    "\n",
    "# Attach embeddings to nodes\n",
    "for node, embedding in zip(nodes, embeddings):\n",
    "    node.embedding = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build index in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes2 = nodes[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One of nodes, objects, or index_struct must be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart indexing at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m total_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/.virtualenvs/doc_retrieval_env/lib/python3.9/site-packages/llama_index/core/indices/vector_store/base.py:76\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     70\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39membed_model\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/doc_retrieval_env/lib/python3.9/site-packages/llama_index/core/indices/base.py:48\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize with parameters.\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m objects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of nodes, objects, or index_struct must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nodes) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one of nodes or index_struct can be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: One of nodes, objects, or index_struct must be provided."
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "index = VectorStoreIndex(storage_context=None)\n",
    "\n",
    "print(f\"Start indexing at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "total_start_time = time.time()\n",
    "for i in range(0, len(nodes2), batch_size):\n",
    "    start_time = time.time()\n",
    "    batch = nodes2[i:i + batch_size]\n",
    "    size_in_bytes = asizeof.asizeof(batch)\n",
    "    print(f\"Batch {i//batch_size + 1} - start time: {datetime.now().strftime('%H:%M:%S')} - size: {format_bytes(size_in_bytes)}\")\n",
    "    \n",
    "    index.insert_nodes(batch)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Batch {i//batch_size + 1} - Indexing time: {format_seconds(elapsed_time)}\")\n",
    "   \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - total_start_time\n",
    "print(f\"Stop indexing at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Total indexing time: {format_seconds(elapsed_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    response_mode=\"no_text\",\n",
    "    similarity_top_k=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response):\n",
    "    source_nodes = response.source_nodes  # List of source nodes\n",
    "\n",
    "    # print(source_nodes[0])\n",
    "\n",
    "    results = [\n",
    "        {\n",
    "            \"title\": node.metadata[\"file_path\"].split('/')[-1],\n",
    "            \"file_path\": node.metadata[\"file_path\"],\n",
    "            \"content\": node.text[:200],\n",
    "            \"score\": node.score,\n",
    "            \"search_type\": 'llamaindex',\n",
    "            # \"metadata\": node.metadata  # Extract metadata (e.g., file paths, tags)\n",
    "        }\n",
    "        for node in source_nodes\n",
    "    ]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_results(query, results):\n",
    "    print(f\"query: {query}\")\n",
    "    for idx, doc in enumerate(results, start=1):\n",
    "        print(f\"Document {idx}: {doc['title']}\")\n",
    "        # print(f\"Content: {doc['content'][:200]}\")\n",
    "        print(f\"File path: {doc['file_path']}\")\n",
    "        print(f\"Score: {doc['score']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the impact of the GDPR on me?\"\n",
    "response = query_engine.query(query)\n",
    "results = format_response(response)\n",
    "print_results(query, results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"writing index to file\")\n",
    "storage_file = \"llamaindex_stella\"\n",
    "index.storage_context.persist(storage_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_retrieval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
