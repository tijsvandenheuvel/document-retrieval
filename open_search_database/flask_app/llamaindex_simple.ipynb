{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamaindex create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_opensearch import opensearch_client\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "# from pympler import asizeof\n",
    "# from llama_index.core.node_parser import SimpleNodeParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents_from_opensearch(client, index_name, num_of_docs):\n",
    "    search_body = {\n",
    "        \"query\": {\"match_all\": {}},  # Fetch all documents\n",
    "        \"_source\": [\"content\", \"content_vector\", \"file_path\"]\n",
    "    }\n",
    "    response = client.search(index=index_name, body=search_body, size=num_of_docs)\n",
    "\n",
    "    documents = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        source = hit['_source']\n",
    "        documents.append({\n",
    "            \"content\": source[\"content\"],\n",
    "            \"vector\": source[\"content_vector\"],  # Vector embeddings\n",
    "            \"metadata\": {\"file_path\": source.get(\"file_path\", \"\")}  # Add any metadata needed\n",
    "        })\n",
    "        \n",
    "    return documents\n",
    "\n",
    "# def format_bytes(size):\n",
    "#     for unit in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "#         if size < 1024.0:\n",
    "#             return f\"{size:.2f} {unit}\"\n",
    "#         size /= 1024.0\n",
    "        \n",
    "# def format_seconds(seconds):\n",
    "#     days = int(seconds // 86400)\n",
    "#     seconds %= 86400\n",
    "#     hours = int(seconds // 3600)\n",
    "#     seconds %= 3600\n",
    "#     minutes = int(seconds // 60)\n",
    "#     seconds %= 60\n",
    "\n",
    "#     result = []\n",
    "#     if days > 0:\n",
    "#         result.append(f\"{days} days\")\n",
    "#     if hours > 0:\n",
    "#         result.append(f\"{hours} hours\")\n",
    "#     if minutes > 0:\n",
    "#         result.append(f\"{minutes} minutes\")\n",
    "#     result.append(f\"{seconds:.2f} seconds\")\n",
    "\n",
    "#     return \", \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config llamindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install llama-index-embeddings-instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = SentenceTransformer(\"sentence-transformers/LaBSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_size = 4096\n",
    "chunk_size = 512\n",
    "Settings.chunk_size = chunk_size\n",
    "\n",
    "Settings.llm = None\n",
    "\n",
    "# model_name=\"dunzhang/stella_en_1.5B_v5\"\n",
    "# model_name=\"BAAI/bge-small-en-v1.5\"\n",
    "# model_name=\"jinaai/jina-embeddings-v3\"\n",
    "\n",
    "# model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "model_name=\"sentence-transformers/LaBSE\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=model_name\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Embedding model: {model_name}\")\n",
    "print(f\"Chunk size: {chunk_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = fetch_documents_from_opensearch(opensearch_client, \"documents\", 10000)\n",
    "    \n",
    "print(f\"Documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_documents = [\n",
    "        Document(\n",
    "            text=doc[\"content\"],\n",
    "            metadata=doc[\"metadata\"]\n",
    "        )\n",
    "        for doc in documents\n",
    "    ]\n",
    "print(len(llama_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LlamaIndex\n",
    "index = VectorStoreIndex.from_documents(llama_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    response_mode=\"no_text\",\n",
    "    similarity_top_k=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine2 = RetrieverQueryEngine(\n",
    "#     retriever=index.as_retriever(\n",
    "#         # response_mode=\"no_text\",\n",
    "#         similarity_top_k=10\n",
    "#         )\n",
    "#     )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response):\n",
    "    source_nodes = response.source_nodes \n",
    "    \n",
    "    grouped_results = {}\n",
    "    for node in source_nodes:\n",
    "        file_path = node.metadata[\"file_path\"]\n",
    "        if file_path not in grouped_results:\n",
    "            grouped_results[file_path] = {\n",
    "                \"title\": file_path.split('/')[-1],\n",
    "                \"file_path\": file_path,\n",
    "                \"content\": node.text[:200],\n",
    "                \"score\": node.score,\n",
    "                \"search_type\": 'llamaindex'\n",
    "            }\n",
    "        else:\n",
    "            # Optionally, update with a higher score or merge content\n",
    "            grouped_results[file_path][\"score\"] = max(grouped_results[file_path][\"score\"], node.score)\n",
    "    return list(grouped_results.values())[:10]\n",
    "\n",
    "def print_results(query, results):\n",
    "    print(f\"query: {query}\")\n",
    "    for idx, doc in enumerate(results, start=1):\n",
    "        print(f\"Document {idx}: {doc['title']}\")\n",
    "        # print(f\"Content: {doc['content'][:200]}\")\n",
    "        # print(f\"File path: {doc['file_path']}\")\n",
    "        # print(f\"Score: {doc['score']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"corruption\"\n",
    "response = query_engine.query(query)\n",
    "results = format_response(response)\n",
    "print_results(query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"writing index to file\")\n",
    "storage_file = \"llamaindex_labse\"\n",
    "index.storage_context.persist(storage_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_retrieval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
