{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamaindex create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tijsvandenheuvel/.virtualenvs/doc_retrieval_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from db_opensearch import opensearch_client\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "# from pympler import asizeof\n",
    "# from llama_index.core.node_parser import SimpleNodeParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents_from_opensearch(client, index_name, num_of_docs):\n",
    "    search_body = {\n",
    "        \"query\": {\"match_all\": {}},  # Fetch all documents\n",
    "        \"_source\": [\"content\", \"content_vector\", \"file_path\"]\n",
    "    }\n",
    "    response = client.search(index=index_name, body=search_body, size=num_of_docs)\n",
    "\n",
    "    documents = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        source = hit['_source']\n",
    "        documents.append({\n",
    "            \"content\": source[\"content\"],\n",
    "            \"vector\": source[\"content_vector\"],  # Vector embeddings\n",
    "            \"metadata\": {\"file_path\": source.get(\"file_path\", \"\")}  # Add any metadata needed\n",
    "        })\n",
    "        \n",
    "    return documents\n",
    "\n",
    "# def format_bytes(size):\n",
    "#     for unit in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "#         if size < 1024.0:\n",
    "#             return f\"{size:.2f} {unit}\"\n",
    "#         size /= 1024.0\n",
    "        \n",
    "# def format_seconds(seconds):\n",
    "#     days = int(seconds // 86400)\n",
    "#     seconds %= 86400\n",
    "#     hours = int(seconds // 3600)\n",
    "#     seconds %= 3600\n",
    "#     minutes = int(seconds // 60)\n",
    "#     seconds %= 60\n",
    "\n",
    "#     result = []\n",
    "#     if days > 0:\n",
    "#         result.append(f\"{days} days\")\n",
    "#     if hours > 0:\n",
    "#         result.append(f\"{hours} hours\")\n",
    "#     if minutes > 0:\n",
    "#         result.append(f\"{minutes} minutes\")\n",
    "#     result.append(f\"{seconds:.2f} seconds\")\n",
    "\n",
    "#     return \", \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config llamindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n",
      "Embedding model: BAAI/bge-small-en-v1.5\n",
      "Chunk size: 4096\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 4096\n",
    "Settings.chunk_size = chunk_size\n",
    "\n",
    "Settings.llm = None\n",
    "\n",
    "# model_name=\"dunzhang/stella_en_1.5B_v5\"\n",
    "model_name=\"BAAI/bge-small-en-v1.5\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(f\"Embedding model: {model_name}\")\n",
    "print(f\"Chunk size: {chunk_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 4695\n"
     ]
    }
   ],
   "source": [
    "documents = fetch_documents_from_opensearch(opensearch_client, \"documents\", 10000)\n",
    "    \n",
    "print(f\"Documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4695\n"
     ]
    }
   ],
   "source": [
    "llama_documents = [\n",
    "        Document(\n",
    "            text=doc[\"content\"],\n",
    "            metadata=doc[\"metadata\"]\n",
    "        )\n",
    "        for doc in documents\n",
    "    ]\n",
    "print(len(llama_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LlamaIndex\n",
    "index = VectorStoreIndex.from_documents(llama_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    response_mode=\"no_text\",\n",
    "    similarity_top_k=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response):\n",
    "    source_nodes = response.source_nodes  # List of source nodes\n",
    "\n",
    "    # print(source_nodes[0])\n",
    "\n",
    "    results = [\n",
    "        {\n",
    "            \"title\": node.metadata[\"file_path\"].split('/')[-1],\n",
    "            \"file_path\": node.metadata[\"file_path\"],\n",
    "            \"content\": node.text[:200],\n",
    "            \"score\": node.score,\n",
    "            \"search_type\": 'llamaindex',\n",
    "            # \"metadata\": node.metadata  # Extract metadata (e.g., file paths, tags)\n",
    "        }\n",
    "        for node in source_nodes\n",
    "    ]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_results(query, results):\n",
    "    print(f\"query: {query}\")\n",
    "    for idx, doc in enumerate(results, start=1):\n",
    "        print(f\"Document {idx}: {doc['title']}\")\n",
    "        # print(f\"Content: {doc['content'][:200]}\")\n",
    "        # print(f\"File path: {doc['file_path']}\")\n",
    "        # print(f\"Score: {doc['score']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the impact of the GDPR on me?\"\n",
    "response = query_engine.query(query)\n",
    "results = format_response(response)\n",
    "print_results(query, results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing index to file\n"
     ]
    }
   ],
   "source": [
    "print(\"writing index to file\")\n",
    "storage_file = \"llamaindex_bge_small\"\n",
    "index.storage_context.persist(storage_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: Kunnen bedrijven justitiële gegevens verwerken om corruptie te bestrijden?\n",
      "Document 1: advies_32_1998.pdf\n",
      "\n",
      "Document 2: aanbeveling_02_2016_0.pdf\n",
      "\n",
      "Document 3: advies_07_1993.pdf\n",
      "\n",
      "Document 4: beraadslaging_RR_52_2017_0.pdf\n",
      "\n",
      "Document 5: AD110-2019.pdf\n",
      "\n",
      "Document 6: advies_02_1992.pdf\n",
      "\n",
      "Document 7: AVG (de nieuwe Europese privacywet).pdf\n",
      "\n",
      "Document 8: wp248_rev.01_nl.pdf\n",
      "\n",
      "Document 9: 12862010_Privacyrecht_Dumortier_VRGAlumni 2f90.pdf\n",
      "\n",
      "Document 10: Beslissing_GK_22-2020_NL.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Kunnen bedrijven justitiële gegevens verwerken om corruptie te bestrijden?\"\n",
    "response = query_engine.query(query)\n",
    "results = format_response(response)\n",
    "print_results(query, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_retrieval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
