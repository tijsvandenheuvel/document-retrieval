{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamaindex create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tijsvandenheuvel/.virtualenvs/doc_retrieval_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from db_opensearch import opensearch_client\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pympler import asizeof\n",
    "from llama_index.core.node_parser import SimpleNodeParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents_from_opensearch(client, index_name, num_of_docs):\n",
    "    search_body = {\n",
    "        \"query\": {\"match_all\": {}},  # Fetch all documents\n",
    "        \"_source\": [\"content\", \"content_vector\", \"file_path\"]\n",
    "    }\n",
    "    response = client.search(index=index_name, body=search_body, size=num_of_docs)\n",
    "\n",
    "    documents = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        source = hit['_source']\n",
    "        documents.append({\n",
    "            \"content\": source[\"content\"],\n",
    "            \"vector\": source[\"content_vector\"],  # Vector embeddings\n",
    "            \"metadata\": {\"file_path\": source.get(\"file_path\", \"\")}  # Add any metadata needed\n",
    "        })\n",
    "        \n",
    "    return documents\n",
    "\n",
    "def format_bytes(size):\n",
    "    for unit in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size < 1024.0:\n",
    "            return f\"{size:.2f} {unit}\"\n",
    "        size /= 1024.0\n",
    "        \n",
    "def format_seconds(seconds):\n",
    "    days = int(seconds // 86400)\n",
    "    seconds %= 86400\n",
    "    hours = int(seconds // 3600)\n",
    "    seconds %= 3600\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds %= 60\n",
    "\n",
    "    result = []\n",
    "    if days > 0:\n",
    "        result.append(f\"{days} days\")\n",
    "    if hours > 0:\n",
    "        result.append(f\"{hours} hours\")\n",
    "    if minutes > 0:\n",
    "        result.append(f\"{minutes} minutes\")\n",
    "    result.append(f\"{seconds:.2f} seconds\")\n",
    "\n",
    "    return \", \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config llamindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n",
      "Embedding model: dunzhang/stella_en_1.5B_v5\n",
      "Chunk size: 4096\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 4096\n",
    "Settings.chunk_size = chunk_size\n",
    "\n",
    "Settings.llm = None\n",
    "\n",
    "model_name=\"dunzhang/stella_en_1.5B_v5\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(f\"Embedding model: {model_name}\")\n",
    "print(f\"Chunk size: {chunk_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 4695\n"
     ]
    }
   ],
   "source": [
    "documents = fetch_documents_from_opensearch(opensearch_client, \"documents\", 10000)\n",
    "    \n",
    "print(f\"Documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4695\n",
      "46814\n"
     ]
    }
   ],
   "source": [
    "llama_documents = [\n",
    "        Document(\n",
    "            text=doc[\"content\"],\n",
    "            metadata=doc[\"metadata\"]\n",
    "        )\n",
    "        for doc in documents\n",
    "    ]\n",
    "print(len(llama_documents))\n",
    "\n",
    "node_parser = SimpleNodeParser()\n",
    "nodes = node_parser.get_nodes_from_documents(llama_documents)\n",
    "\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "embedding_model = HuggingFaceEmbedding(model_name=model_name)\n",
    "\n",
    "print(dir(embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all nodes\n",
    "node_texts = [node.text for node in nodes]\n",
    "embeddings = embedding_model.embed_documents(node_texts)\n",
    "\n",
    "# Attach embeddings to nodes\n",
    "for node, embedding in zip(nodes, embeddings):\n",
    "    node.embedding = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build index in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "index = VectorStoreIndex(storage_context=None)\n",
    "\n",
    "print(f\"Start indexing at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "total_start_time = time.time()\n",
    "for i in range(0, len(nodes), batch_size):\n",
    "    start_time = time.time()\n",
    "    batch = nodes[i:i + batch_size]\n",
    "    size_in_bytes = asizeof.asizeof(batch)\n",
    "    print(f\"Batch {i//batch_size + 1} - start time: {datetime.now().strftime('%H:%M:%S')} - size: {format_bytes(size_in_bytes)}\")\n",
    "    \n",
    "    index.insert_nodes(batch)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Batch {i//batch_size + 1} - Indexing time: {format_seconds(elapsed_time)}\")\n",
    "   \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - total_start_time\n",
    "print(f\"Stop indexing at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Total indexing time: {format_seconds(elapsed_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    response_mode=\"no_text\",\n",
    "    similarity_top_k=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response):\n",
    "    source_nodes = response.source_nodes  # List of source nodes\n",
    "\n",
    "    # print(source_nodes[0])\n",
    "\n",
    "    results = [\n",
    "        {\n",
    "            \"title\": node.metadata[\"file_path\"].split('/')[-1],\n",
    "            \"file_path\": node.metadata[\"file_path\"],\n",
    "            \"content\": node.text[:200],\n",
    "            \"score\": node.score,\n",
    "            \"search_type\": 'llamaindex',\n",
    "            # \"metadata\": node.metadata  # Extract metadata (e.g., file paths, tags)\n",
    "        }\n",
    "        for node in source_nodes\n",
    "    ]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_results(query, results):\n",
    "    print(f\"query: {query}\")\n",
    "    for idx, doc in enumerate(results, start=1):\n",
    "        print(f\"Document {idx}: {doc['title']}\")\n",
    "        # print(f\"Content: {doc['content'][:200]}\")\n",
    "        print(f\"File path: {doc['file_path']}\")\n",
    "        print(f\"Score: {doc['score']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the impact of the GDPR on me?\"\n",
    "response = query_engine.query(query)\n",
    "results = format_response(response)\n",
    "print_results(query, results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"writing index to file\")\n",
    "storage_file = \"llamaindex_stella\"\n",
    "index.storage_context.persist(storage_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_retrieval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
